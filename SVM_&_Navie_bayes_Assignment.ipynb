{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical Questions**"
      ],
      "metadata": {
        "id": "y9Z21pXsG-kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. What is a Support Vector Machine (SVM)?\n",
        " ** A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates data points of different classes in a feature space. The goal is to maximize the margin between the classes, which helps improve generalization. SVM is effective in high-dimensional spaces and works well when the number of features exceeds the number of samples. It can also handle non-linear classification using kernel tricks that transform data into higher dimensions. SVMs are widely used in image recognition, bioinformatics, and text classification.\n"
      ],
      "metadata": {
        "id": "KE3aCMFBHEQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is the difference between Hard Margin and Soft Margin SVM?\n",
        "** Hard Margin SVM is used when the data is perfectly linearly separable. It creates a decision boundary that does not allow any misclassification, aiming for the maximum margin between classes. However, it is sensitive to outliers and noise. Soft Margin SVM is more flexible and allows some misclassifications to achieve better generalization on unseen data. It introduces a penalty for incorrect classifications, making it suitable for real-world datasets that are not perfectly separable. Soft Margin SVM balances margin maximization and error minimization, making it more robust and widely applicable than Hard Margin SVM.\n"
      ],
      "metadata": {
        "id": "sK7X_JBPHPtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is the mathematical intuition behind SVM?\n",
        "** The mathematical intuition behind SVM lies in finding the hyperplane that best separates data points of different classes by maximizing the margin between them. The margin is the distance between the hyperplane and the nearest data points from each class, known as support vectors. SVM aims to solve an optimization problem that minimizes the norm of the weight vector while ensuring correct classification of the training data. In cases where data is not linearly separable, a kernel function transforms it into a higher-dimensional space, making separation possible. This approach helps SVM achieve strong performance, even in complex classification tasks.\n"
      ],
      "metadata": {
        "id": "AyeqCDPAHixH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is the role of Lagrange Multipliers in SVM?\n",
        "** Lagrange Multipliers play a key role in solving the optimization problem in SVM. They help convert the constrained optimization problem of finding the maximum margin hyperplane into a form that is easier to solve. By using the Lagrangian, constraints on the data points being correctly classified are incorporated into the objective function. This allows the problem to be expressed in its dual form, which depends only on the inner products of data points. It also helps identify the support vectors, as only data points with non-zero multipliers influence the final decision boundary, making the model efficient and effective.\n"
      ],
      "metadata": {
        "id": "J1cyVhHiHyAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What are Support Vectors in SVM?\n",
        "** Support Vectors are the data points in a dataset that lie closest to the decision boundary (or hyperplane) in a Support Vector Machine (SVM) model. They are the most critical elements of the training set because they directly influence the position and orientation of the optimal hyperplane. The margin, which SVM aims to maximize, is measured based on the distance between these support vectors and the hyperplane. Only these points are used in defining the decision function; all other points have no effect. As a result, support vectors help SVM generalize well and make efficient predictions.\n"
      ],
      "metadata": {
        "id": "Wuq1CMJtH9PS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  What is a Support Vector Classifier (SVC)?\n",
        "** A Support Vector Classifier (SVC) is a type of Support Vector Machine (SVM) used specifically for classification tasks. It finds the optimal hyperplane that best separates data points from different classes with the maximum possible margin. SVC can handle both linearly and non-linearly separable data using kernel functions, which map data into higher-dimensional spaces where separation is easier. It allows some misclassifications through the use of a soft margin, helping it perform well on noisy or overlapping datasets. SVC is widely used in applications like image recognition, text classification, and bioinformatics due to its accuracy and robustness.\n"
      ],
      "metadata": {
        "id": "HHfaZn0aIFV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is a Support Vector Regressor (SVR)?\n",
        "** A Support Vector Regressor (SVR) is a type of Support Vector Machine (SVM) used for regression tasks. Unlike classification, SVR aims to predict continuous values. It tries to find a function that approximates the target values within a certain margin of tolerance, called epsilon. The model only considers data points that fall outside this margin, known as support vectors. SVR balances model complexity and prediction error by minimizing the weight vector while allowing some flexibility through slack variables. It is effective in high-dimensional spaces and performs well even when the relationship between features and target is non-linear using kernels.\n"
      ],
      "metadata": {
        "id": "eapnOBk6IPkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is the Kernel Trick in SVM?\n",
        "** The Kernel Trick in SVM is a mathematical technique that allows the algorithm to operate in a high-dimensional space without explicitly transforming the data. Instead of mapping data points into a higher-dimensional space directly, the kernel function computes the inner products between pairs of data points as if they were transformed. This enables SVM to learn complex, non-linear decision boundaries efficiently. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid. The Kernel Trick makes it possible to solve problems that are not linearly separable in the original feature space while keeping computations manageable.\n"
      ],
      "metadata": {
        "id": "sKU_c0oCIX_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n",
        "** The Linear Kernel is best for linearly separable data and is fast and efficient, especially with high-dimensional features. The Polynomial Kernel captures curved relationships by applying polynomial transformations, making it suitable for moderately complex data, though it can be slower and prone to overfitting. The RBF (Radial Basis Function) Kernel handles highly non-linear patterns by mapping data into infinite-dimensional space using a Gaussian function. It is very flexible but requires careful tuning of parameters like gamma. Each kernel serves different data complexities, making kernel selection crucial for optimal model performance in SVM-based tasks.\n"
      ],
      "metadata": {
        "id": "6Z0temZeIhsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the effect of the C parameter in SVM?\n",
        "** The C parameter in SVM controls the trade-off between achieving a low training error and maintaining a large margin. It acts as a regularization parameter:\n",
        "\n",
        "* A high C value tries to minimize classification errors by giving more importance to correctly classifying all training examples. This leads to less margin and potentially overfitting, as the model may become too sensitive to noise and outliers.\n",
        "\n",
        "* A low C value allows more misclassifications but focuses on finding a wider margin. This encourages better generalization and can reduce overfitting, though it may increase training error.\n",
        "\n",
        "In short, C balances model complexity and accuracy.\n"
      ],
      "metadata": {
        "id": "NcvbBU8AIuVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "** The Gamma parameter in an RBF (Radial Basis Function) Kernel SVM defines how far the influence of a single training point reaches:\n",
        "\n",
        "* A high gamma value means each point has a narrow influence, creating very tight decision boundaries. This can lead to overfitting, as the model becomes too sensitive to individual data points.\n",
        "\n",
        "* A low gamma value means each point has a wide influence, resulting in smoother, broader decision boundaries. This can lead to underfitting if the model is too simple to capture data patterns.\n",
        "\n",
        "In essence, gamma controls the curvature of the decision boundaryin RBF SVM.\n"
      ],
      "metadata": {
        "id": "CzyvQIsYJJkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve?\n",
        "** The Na√Øve Bayes classifier is a supervised machine learning algorithm based on Bayes‚Äô Theorem, used for classification tasks. It calculates the probability of each class given a set of features and assigns the label with the highest probability.\n",
        "\n",
        "It is called \"Na√Øve\" because it makes a strong assumption that all features are independent of each other given the class label, which is rarely true in real-world data. Despite this simplification, Na√Øve Bayes often performs surprisingly well, especially in high-dimensional problems like text classification, spam detection, and sentiment analysis, due to its simplicity and speed.\n"
      ],
      "metadata": {
        "id": "E_TDcCt6lPvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Bayes‚Äô Theorem?\n",
        "** Bayes‚Äô Theorem is a mathematical formula used to calculate the probability of an event based on prior knowledge of conditions related to that event. In machine learning, it helps update the probability of a hypothesis as more evidence or data becomes available. It combines prior probability with the likelihood of the observed data to produce a posterior probability. This theorem is the foundation of the Na√Øve Bayes classifier. It enables models to make predictions even with limited data and is especially useful in classification problems like spam detection, medical diagnosis, and text classification tasks.\n"
      ],
      "metadata": {
        "id": "LamRwENclh3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes?\n",
        "** Gaussian, Multinomial, and Bernoulli Na√Øve Bayes are variants suited for different data types. Gaussian Na√Øve Bayes is used when features are continuous and assumes data follows a normal distribution, making it ideal for tasks like medical diagnosis. Multinomial Na√Øve Bayes works well with discrete features such as word counts in text classification, where data represents frequency. Bernoulli Na√Øve Bayes is designed for binary data, assuming features are either present or absent, commonly used in email spam detection. Each variant applies Bayes' Theorem with different assumptions about the feature distribution, helping choose the right model based on data characteristics.\n"
      ],
      "metadata": {
        "id": "ZcZHOjFXluN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should you use Gaussian Na√Øve Bayes over other variants?\n",
        "** Use Gaussian Na√Øve Bayes when your dataset contains continuous numerical features that follow a normal (Gaussian) distribution. It is ideal for problems where input variables, like age, income, or sensor readings, are real-valued and not discrete. This variant is commonly used in fields such as medical diagnosis, image classification, and real-time predictions due to its simplicity, speed, and efficiency. Gaussian Na√Øve Bayes assumes each feature is normally distributed within each class. It is preferred over Multinomial or Bernoulli Na√Øve Bayes when the data is not count-based or binary, making it highly suitable for real-world numerical classification tasks.\n"
      ],
      "metadata": {
        "id": "FdGgVZsSmCqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the key assumptions made by Na√Øve Bayes?\n",
        "** Na√Øve Bayes makes three key assumptions. First, it assumes feature independence, meaning each feature contributes to the outcome independently, even though this is rarely true in real-world data. Second, it assumes class conditional independence, where the presence of one feature does not affect the probability of another, given the class. Third, it assumes a specific distribution based on the variant used: Gaussian for continuous data, Multinomial for count-based features, and Bernoulli for binary features. These simplifying assumptions make Na√Øve Bayes highly efficient and scalable for classification tasks, despite its ‚Äúna√Øve‚Äù approach to feature relationships.\n"
      ],
      "metadata": {
        "id": "va0I47E-mo5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  What are the advantages and disadvantages of Na√Øve Bayes?\n",
        "** Na√Øve Bayes is a simple, fast, and efficient classification algorithm, especially suitable for high-dimensional data like text. It performs well with small datasets, handles irrelevant features, and is robust to noise. Its ease of implementation makes it a popular choice for baseline models. However, it relies on the strong assumption that all features are independent given the class label, which is rarely true in real-world scenarios. It also faces the zero-probability problem when encountering unseen features, though this can be mitigated with smoothing. Additionally, its probability estimates may be inaccurate, and it struggles with complex feature interactions.\n"
      ],
      "metadata": {
        "id": "hJGR_-YInY1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  Why is Na√Øve Bayes a good choice for text classification?\n",
        "** Na√Øve Bayes is a good choice for text classification due to its simplicity, efficiency, and strong performance in high-dimensional spaces. Text data often has thousands of features (words or tokens), and Na√Øve Bayes handles this well because it assumes feature independence, which reduces computational complexity. It performs particularly well with word frequency or presence data, making it ideal for spam detection, sentiment analysis, and document categorization. It requires less training data, trains quickly, and works effectively even when data is noisy or sparse. Despite its ‚Äúna√Øve‚Äù assumptions, it often matches or outperforms more complex models in text tasks.\n"
      ],
      "metadata": {
        "id": "Ua3HIBDFoCAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Compare SVM and Na√Øve Bayes for classification tasks?\n",
        "** Na√Øve Bayes and SVM are popular classification algorithms with distinct strengths. Na√Øve Bayes is fast, simple, and works well with high-dimensional data like text. It assumes feature independence and provides probabilistic outputs, making it ideal for spam detection and sentiment analysis. In contrast, SVM finds the optimal hyperplane to separate classes and performs better with complex, non-linear data. It is more accurate in many cases but computationally heavier and harder to interpret. SVM doesn‚Äôt natively offer probabilities. Choose Na√Øve Bayes for speed and simplicity, and SVM when accuracy and handling complex boundaries are priorities.\n"
      ],
      "metadata": {
        "id": "VJPCpkLsoTIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How does Laplace Smoothing help in Na√Øve Bayes?\n",
        "** Laplace Smoothing, also known as add-one smoothing, is used in Na√Øve Bayes to address the zero-frequency problem, which occurs when a word or feature in the test data is not present in the training data for a given class. Without smoothing, this results in a zero probability, which can invalidate the entire probability calculation. Laplace Smoothing fixes this by adding one to each word count and adjusting the denominator accordingly, ensuring no probability is ever zero. This helps the model generalize better, especially in sparse datasets like text classification, where many rare or unseen words can affect predictions.\n"
      ],
      "metadata": {
        "id": "4s5CEyGmomuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical Questions**"
      ],
      "metadata": {
        "id": "Zq-LX_Vtoziw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "21.  Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy.\n",
        "** # Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM classifier (using RBF kernel by default)\n",
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"SVM Classifier Accuracy on Iris Dataset: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "id": "P24wgn-spKu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "compare their accuracies.\n",
        "** # Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', C=1.0)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"SVM with Linear Kernel Accuracy: {:.2f}%\".format(accuracy_linear * 100))\n",
        "print(\"SVM with RBF Kernel Accuracy: {:.2f}%\".format(accuracy_rbf * 100))\n"
      ],
      "metadata": {
        "id": "IK5kp1BnpbPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "23. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "compare their accuracies.\n",
        "** # Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', C=1.0)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"SVM with Linear Kernel Accuracy: {:.2f}%\".format(accuracy_linear * 100))\n",
        "print(\"SVM with RBF Kernel Accuracy: {:.2f}%\".format(accuracy_rbf * 100))\n"
      ],
      "metadata": {
        "id": "v8pOopl4pwKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "24. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "Squared Error (MSE).\n",
        "** # Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Standardize the data (important for SVR)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVR model (RBF kernel by default)\n",
        "svr_model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_scaled = svr_model.predict(X_test)\n",
        "\n",
        "# Inverse transform to get predictions in original scale\n",
        "y_test_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
        "y_pred_orig = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
        "\n",
        "# Print the result\n",
        "print(\"SVR Mean Squared Error on California Housing dataset: {:.4f}\".format(mse))\n"
      ],
      "metadata": {
        "id": "euT0NBz3p_1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "25.  Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and\n",
        "evaluate accuracy.\n",
        "** # Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gaussian Na√Øve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print result\n",
        "print(\"Gaussian Na√Øve Bayes Classifier Accuracy on Breast Cancer Dataset: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "id": "IlUNkEByqIgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "26. Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20\n",
        "Newsgroups dataset.\n",
        "** # Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gaussian Na√Øve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print result\n",
        "print(\"Gaussian Na√Øve Bayes Classifier Accuracy on Breast Cancer Dataset: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "id": "pBsvb4ryqS6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "27. Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "boundaries visually.\n",
        "** import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a synthetic 2D classification dataset\n",
        "X, y = make_classification(n_samples=300, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of C values to compare\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "# Set up the plot\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Create mesh grid for plotting decision boundaries\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
        "                     np.linspace(y_min, y_max, 500))\n",
        "\n",
        "# Train and plot for each C value\n",
        "for i, C in enumerate(C_values):\n",
        "    clf = SVC(kernel='linear', C=C)\n",
        "    clf.fit(X_train, y_train)\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.title(f\"SVM Decision Boundary (C={C})\")\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZRqwVQWCqmtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "28. Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with\n",
        "binary features.\n",
        "** # Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=10, n_redundant=0,\n",
        "                           n_classes=2, random_state=42)\n",
        "\n",
        "# Binarize features (threshold = 0.0)\n",
        "binarizer = Binarizer(threshold=0.0)\n",
        "X_binary = binarizer.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_binary, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Bernoulli Na√Øve Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Bernoulli Na√Øve Bayes Classifier Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "qw88D5S2qvYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "29. Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "unscaled data.\n",
        "** from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --------- SVM without scaling ---------\n",
        "svm_unscaled = SVC(kernel='rbf')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# --------- Apply feature scaling ---------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --------- SVM with scaling ---------\n",
        "svm_scaled = SVC(kernel='rbf')\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# --------- Print Results ---------\n",
        "print(\"SVM Accuracy without Scaling: {:.2f}%\".format(accuracy_unscaled * 100))\n",
        "print(\"SVM Accuracy with Scaling: {:.2f}%\".format(accuracy_scaled * 100))\n"
      ],
      "metadata": {
        "id": "zDT95Pf1q_-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "30.  Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and\n",
        "after Laplace Smoothing.\n",
        "** from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the digits dataset (suitable for Multinomial NB)\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Normalize features to be count-like (0-16 pixel values)\n",
        "X = (X / X.max()) * 10\n",
        "X = X.astype(int)\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without Laplace Smoothing (alpha=0.0)\n",
        "model_no_smooth = MultinomialNB(alpha=0.0)\n",
        "model_no_smooth.fit(X_train, y_train)\n",
        "y_pred_no_smooth = model_no_smooth.predict(X_test)\n",
        "acc_no_smooth = accuracy_score(y_test, y_pred_no_smooth)\n",
        "\n",
        "# With Laplace Smoothing (alpha=1.0)\n",
        "model_laplace = MultinomialNB(alpha=1.0)\n",
        "model_laplace.fit(X_train, y_train)\n",
        "y_pred_laplace = model_laplace.predict(X_test)\n",
        "acc_laplace = accuracy_score(y_test, y_pred_laplace)\n",
        "\n",
        "# Results\n",
        "print(\"Accuracy WITHOUT Laplace Smoothing (alpha=0.0): {:.2f}%\".format(acc_no_smooth * 100))\n",
        "print(\"Accuracy WITH Laplace Smoothing (alpha=1.0): {:.2f}%\".format(acc_laplace * 100))\n"
      ],
      "metadata": {
        "id": "-dknr4F0rMob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "gamma, kernel).\n",
        "** from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "\n",
        "# Create and run GridSearchCV\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and estimator\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = grid.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy with Best Parameters: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "id": "bbt2LiM3rWCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "check it improve accuracy.\n",
        "** from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n",
        "                           n_redundant=10, n_clusters_per_class=1,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Step 2: Split into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM without class weights\n",
        "svm_default = SVC(kernel='rbf')\n",
        "svm_default.fit(X_train, y_train)\n",
        "y_pred_default = svm_default.predict(X_test)\n",
        "print(\"Without Class Weighting:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_default))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_default))\n",
        "\n",
        "# Step 4: Train SVM with class_weight='balanced'\n",
        "svm_weighted = SVC(kernel='rbf', class_weight='balanced')\n",
        "svm_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = svm_weighted.predict(X_test)\n",
        "print(\"\\nWith Class Weighting:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_weighted))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_weighted))\n"
      ],
      "metadata": {
        "id": "R123xAX1rib6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "33.  Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data.\n",
        "** from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Sample email dataset (text, label)\n",
        "emails = [\n",
        "    (\"Win a free iPhone now\", \"spam\"),\n",
        "    (\"Limited time offer, click here!\", \"spam\"),\n",
        "    (\"Meeting at 10am tomorrow\", \"ham\"),\n",
        "    (\"Lunch plans?\", \"ham\"),\n",
        "    (\"Get cash instantly, apply now\", \"spam\"),\n",
        "    (\"Your invoice is attached\", \"ham\"),\n",
        "    (\"Congratulations! You've been selected\", \"spam\"),\n",
        "    (\"Let's catch up soon\", \"ham\"),\n",
        "    (\"Earn money from home\", \"spam\"),\n",
        "    (\"Don't forget the team meeting\", \"ham\")\n",
        "]\n",
        "\n",
        "# Step 1: Separate data and labels\n",
        "texts, labels = zip(*emails)\n",
        "\n",
        "# Step 2: Convert text to feature vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Step 3: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Na√Øve Bayes classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Nd5U3SFWrtbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "34. Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and\n",
        "compare their accuracy.\n",
        "** from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature Scaling for SVM (not required for Naive Bayes)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm_model = SVC(kernel='rbf', C=1.0)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "svm_preds = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Train Gaussian Na√Øve Bayes Classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_preds = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate Accuracy\n",
        "svm_accuracy = accuracy_score(y_test, svm_preds)\n",
        "nb_accuracy = accuracy_score(y_test, nb_preds)\n",
        "\n",
        "# Print results\n",
        "print(\"SVM Classifier Accuracy: {:.2f}%\".format(svm_accuracy * 100))\n",
        "print(\"Na√Øve Bayes Classifier Accuracy: {:.2f}%\".format(nb_a_\n"
      ],
      "metadata": {
        "id": "vaVVvB-gr4xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "35. Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare\n",
        "results.\n",
        "** from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ---- Model without Feature Selection ----\n",
        "model_full = GaussianNB()\n",
        "model_full.fit(X_train, y_train)\n",
        "y_pred_full = model_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# ---- Feature Selection using chi-squared test ----\n",
        "selector = SelectKBest(score_func=chi2, k=2)  # Keep top 2 features\n",
        "X_train_sel = selector.fit_transform(X_train, y_train)\n",
        "X_test_sel = selector.transform(X_test)\n",
        "\n",
        "# ---- Model with Selected Features ----\n",
        "model_sel = GaussianNB()\n",
        "model_sel.fit(X_train_sel, y_train)\n",
        "y_pred_sel = model_sel.predict(X_test_sel)\n",
        "acc_sel = accuracy_score(y_test, y_pred_sel)\n",
        "\n",
        "# ---- Output Results ----\n",
        "print(\"Accuracy without Feature Selection: {:.2f}%\".format(acc_full * 100))\n",
        "print(\"Accuracy with Feature Selection (Top 2 features): {:.2f}%\".format(acc_sel * 100))*_\n"
      ],
      "metadata": {
        "id": "rKJa_z8FsB_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "strategies on the Wine dataset and compare their accuracy.\n",
        "** from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# SVM base classifier\n",
        "svm = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "\n",
        "# One-vs-Rest (OvR)\n",
        "ovr_clf = OneVsRestClassifier(svm)\n",
        "ovr_clf.fit(X_train_scaled, y_train)\n",
        "y_pred_ovr = ovr_clf.predict(X_test_scaled)\n",
        "acc_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "# One-vs-One (OvO)\n",
        "ovo_clf = OneVsOneClassifier(svm)\n",
        "ovo_clf.fit(X_train_scaled, y_train)\n",
        "y_pred_ovo = ovo_clf.predict(X_test_scaled)\n",
        "acc_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracy using One-vs-Rest (OvR): {:.2f}%\".format(acc_ovr * 100))\n",
        "print(\"Accuracy using One-vs-One (OvO): {:.2f}%\".format(acc_ovo * 100))\n"
      ],
      "metadata": {
        "id": "hjM29kc8sKNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "Cancer dataset and compare their accuracy.\n",
        "** from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with Polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3)\n",
        "svm_poly.fit(X_train_scaled, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test_scaled)\n",
        "acc_poly = accuracy_score(y_test, y_pred_poly)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy with Linear Kernel: {:.2f}%\".format(acc_linear * 100))\n",
        "print(\"Accuracy with Polynomial Kernel: {:.2f}%\".format(acc_poly * 100))\n",
        "print(\"Accuracy with RBF Kernel: {:.2f}%\".format(acc_rbf * 100))\n"
      ],
      "metadata": {
        "id": "spSligQasSxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "average accuracy.\n",
        "** from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Set up Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "accuracies = []\n",
        "\n",
        "# SVM model\n",
        "svm = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "\n",
        "# Cross-validation loop\n",
        "for train_index, test_index in skf.split(X_scaled, y):\n",
        "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    svm.fit(X_train, y_train)\n",
        "    y_pred = svm.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Print results\n",
        "print(\"Fold Accuracies:\", [\"{:.2f}%\".format(a * 100) for a in accuracies])\n",
        "print(\"Average Accuracy: {:.2f}%\".format(np.mean(accuracies) * 100))\n"
      ],
      "metadata": {
        "id": "DR_YhC15soJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "39.  Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare\n",
        "performance.\n",
        "** from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Define different priors\n",
        "default_prior = None  # automatic priors\n",
        "custom_prior_1 = [0.3, 0.7]  # Prior belief: class 0 (benign) 30%, class 1 (malignant) 70%\n",
        "custom_prior_2 = [0.5, 0.5]  # Equal priors\n",
        "\n",
        "# Function to train and evaluate\n",
        "def train_and_evaluate(priors, label):\n",
        "    model = GaussianNB(priors=priors)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n--- {label} ---\")\n",
        "    print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "    print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "# Train and evaluate with different priors\n",
        "train_and_evaluate(default_prior, \"Default Prior (learned from data)\")\n",
        "train_and_evaluate(custom_prior_1, \"Custom Prior [0.3, 0.7]\")\n",
        "train_and_evaluate(custom_prior_2, \"Custom Prior [0.5, 0.5]\")\n"
      ],
      "metadata": {
        "id": "OUu1-XFuswI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "compare accuracy.\n",
        "** from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Baseline SVM without feature selection\n",
        "svm_baseline = SVC(kernel='linear', random_state=42)\n",
        "svm_baseline.fit(X_train, y_train)\n",
        "y_pred_baseline = svm_baseline.predict(X_test)\n",
        "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
        "\n",
        "# Recursive Feature Elimination\n",
        "rfe = RFE(estimator=SVC(kernel='linear'), n_features_to_select=10)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Apply RFE to train and test sets\n",
        "X_train_rfe = rfe.transform(X_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "# Train SVM with selected features\n",
        "svm_rfe = SVC(kernel='linear', random_state=42)\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "\n",
        "# Print comparison results\n",
        "print(f\"Accuracy without RFE: {accuracy_baseline * 100:.2f}%\")\n",
        "print(f\"Accuracy with RFE (10 features): {accuracy_rfe * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "_ky5xoxWs5ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "41.  Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "F1-Score instead of accuracy.\n",
        "** from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train SVM classifier\n",
        "svm = SVC(kernel='linear', random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "report = classification_report(y_test, y_pred, target_names=data.target_names)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "56cef1kmtFey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "42. Write a Python program to train a Na√Øve Bayes Classifier and evaluate its performance using Log Loss\n",
        "(Cross-Entropy Loss)\n",
        "** from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Gaussian Na√Øve Bayes classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probability scores\n",
        "y_proba = nb_model.predict_proba(X_test)\n",
        "\n",
        "# Calculate Log Loss\n",
        "logloss = log_loss(y_test, y_proba)\n",
        "print(f\"Log Loss (Cross-Entropy Loss): {logloss:.4f}\")\n"
      ],
      "metadata": {
        "id": "2ZlzGT03tNm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn.\n",
        "** import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train an SVM classifier\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - SVM on Iris Dataset')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "u49OBsDstXFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "44.  Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
        "Error (MAE) instead of MSE.\n",
        "** import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load California Housing dataset (modern alternative to deprecated Boston dataset)\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVR model\n",
        "svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
        "svr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = svr.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate with Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error (MAE):\", round(mae, 3))\n"
      ],
      "metadata": {
        "id": "edaO4-yOtd3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "45. Write a Python program to train a Na√Øve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "score.\n",
        "** import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Na√Øve Bayes classifier\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = nb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", round(roc_auc, 3))\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.3f})\", color='blue')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
        "plt.xlabel(\"False Positi\n"
      ],
      "metadata": {
        "id": "oT1EhaRbtlKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve.\n",
        "** import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM classifier with probability estimates\n",
        "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_scores = svm.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate precision, recall, and average precision score\n",
        "precision, recall, thresholds = precision_recall_curve(y_test,_\n"
      ],
      "metadata": {
        "id": "MQQJrv9gtya-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}